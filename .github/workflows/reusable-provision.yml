# ‚ö†Ô∏è SECURITY NOTICE:
# This workflow defaults to GitHub-hosted runners for safety in public repositories.
# Only use runner_type: "self-hosted" in PRIVATE repositories with trusted contributors.
# Fork PRs will require manual approval before running infrastructure operations.

name: "Reusable VM/CT Provisioning"

permissions:
  contents: read

on:
  workflow_call:
    inputs:
      app_name:
        type: string
        description: 'Application Name (e.g. nexus, k3s, octopus)'
        required: true
        default: 'app'
      vlan_tag:
        type: string
        description: 'Network Zone'
        default: '20'
      vm_target_ip:
        type: string
        description: 'Target IP Address (for single instance)'
        required: false
        default: ''
      instances:
        type: string
        description: 'JSON map of instances for multi-instance deployments (e.g., {"node1":{"ip_address":"<INTERNAL_IP_VLAN10>"}}). Leave empty for single instance mode.'
        required: false
        default: ''
      cpu_cores:
        type: string
        description: 'CPU Cores'
        required: true
        default: '2'
      ram_mb:
        type: string
        description: 'RAM in MB'
        required: true
        default: '4096'
      disk_gb:
        type: string
        description: 'Disk Size (e.g. 20G)'
        required: true
        default: '20G'
      # Optional infrastructure overrides
      proxmox_node:
        type: string
        description: 'Proxmox node to deploy on'
        required: false
        default: 'pmx'
      proxmox_storage:
        type: string
        description: 'Storage pool for VM disks'
        required: false
        default: 'zfs-vm'
      vm_template:
        type: string
        description: 'VM template to clone'
        required: false
        default: 'ubuntu-2404-template'
      environment:
        type: string
        description: 'Environment name (dev, staging, prod)'
        required: false
        default: 'dev'
      # Resource type selection
      resource_type:
        type: string
        description: 'Resource type: vm or lxc'
        required: false
        default: 'vm'
      lxc_template:
        type: string
        description: 'LXC template (only for resource_type=lxc)'
        required: false
        default: 'local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.zst'
      lxc_nesting:
        type: boolean
        description: 'Enable nesting for Docker in LXC'
        required: false
        default: false
      lxc_unprivileged:
        type: boolean
        description: 'Run LXC as unprivileged (recommended for security)'
        required: false
        default: true
      # Skip Terraform for existing VMs
      skip_terraform:
        type: boolean
        description: 'Skip Terraform provisioning (use existing VM)'
        required: false
        default: false
      # Octopus Deploy configuration
      octopus_environment:
        type: string
        description: 'Octopus Deploy environment (Development, Staging, Production)'
        required: false
        default: 'Development'
      octopus_roles:
        type: string
        description: 'Comma-separated Octopus roles (e.g., web-server,nginx)'
        required: false
        default: ''
      skip_octopus:
        type: boolean
        description: 'Skip Octopus Tentacle registration'
        required: false
        default: false
      ansible_roles:
        type: string
        description: 'Comma-separated list of Ansible roles to apply (e.g., nginx,mgmt-docker). base_setup always runs by default unless mgmt-docker is specified.'
        required: false
        default: ''
      ansible_extra_vars:
        type: string
        description: 'Additional Ansible extra vars in JSON format (e.g., patroni_backends=[{name:node1,ip:<INTERNAL_IP_VLAN20>}])'
        required: false
        default: ''
      # Runner configuration
      runner_type:
        type: string
        description: 'Runner type: "github-hosted" (uses ubuntu-latest with Tailscale) or "self-hosted" (uses self-hosted runner with direct LAN access). ‚ö†Ô∏è Use self-hosted ONLY in private repositories.'
        required: false
        default: 'github-hosted'
      tailscale_tags:
        type: string
        description: 'Tailscale ACL tags for GitHub-hosted runner (comma-separated, e.g., "tag:ci,tag:provision")'
        required: false
        default: 'tag:ci'
    secrets:
      DOPPLER_TOKEN:
        required: true
      DOPPLER_TARGET_PROJECT:
        required: true
      DOPPLER_TARGET_CONFIG:
        required: true
      GH_PAT:
        required: true
      # Optional: Required for GitHub-hosted runners
      TS_OAUTH_CLIENT_ID:
        required: false
      TS_OAUTH_CLIENT_SECRET:
        required: false

jobs:
  check-runner-access:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      runner: ${{ steps.select-runner.outputs.runner }}
    steps:
      - name: Select runner based on org membership
        id: select-runner
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            const requestedRunner = '${{ inputs.runner_type }}';
            
            // Default to github-hosted
            let runner = 'ubuntu-latest';
            
            // SECURITY: Block external callers from using self-hosted runners
            // For reusable workflows, check the calling repository owner
            const callingRepo = context.payload.repository?.full_name || '';
            const callingOwner = callingRepo.split('/')[0];
            
            core.info(`Calling repository: ${callingRepo}`);
            core.info(`Calling owner: ${callingOwner}`);
            
            // Only allow KoraMaple org repositories to use self-hosted runners
            if (callingOwner !== 'KoraMaple') {
              core.warning('‚ö†Ô∏è External workflow call detected - forcing github-hosted runner for security');
              core.warning(`Called from: ${callingRepo}`);
              core.setOutput('runner', 'ubuntu-latest');
              return;
            }
            
            // Block fork PRs from self-hosted
            if (context.eventName === 'pull_request' || context.eventName === 'pull_request_target') {
              const prRepo = context.payload.pull_request?.head?.repo?.full_name;
              if (prRepo != `${context.repo.owner}/${context.repo.repo}`) {
                core.warning('Fork PRs use GitHub-hosted runners only');
                core.setOutput('runner', 'ubuntu-latest');
                return;
              }
            }
            
            // If self-hosted requested, verify org membership
            if (requestedRunner === 'self-hosted') {
              if (context.repo.owner != 'KoraMaple') {
                core.warning('Self-hosted only available for KoraMaple org repos');
                core.setOutput('runner', 'ubuntu-latest');
                return;
              }
              
              try {
                const { data } = await github.rest.orgs.getMembershipForUser({
                  org: 'KoraMaple',
                  username: context.actor
                });
                if (data.state === 'active') {
                  core.info(`‚úì ${context.actor} is KoraMaple org member - self-hosted allowed`);
                  runner = 'self-hosted';
                }
              } catch (e) {
                core.warning(`${context.actor} is not a KoraMaple org member - using GitHub-hosted`);
              }
            }
            
            core.setOutput('runner', runner);

  provision:
    needs: check-runner-access
    runs-on: ${{ needs.check-runner-access.outputs.runner }}
    # Prevent fork PRs from running infrastructure operations - maintainers must explicitly trigger
    if: |
      github.event_name != 'pull_request' ||
      github.event.pull_request.head.repo.full_name == github.repository
    
    steps:
      - uses: actions/checkout@v4

      - name: Debug Tailscale secrets presence
        if: needs.check-runner-access.outputs.runner == 'ubuntu-latest'
        run: |
          echo "ID='${{ secrets.TS_OAUTH_CLIENT_ID }}'"
          echo "SECRET set? ${{ secrets.TS_OAUTH_CLIENT_SECRET != '' }}"
          if [ -z "${{ secrets.TS_OAUTH_CLIENT_ID }}" ]; then
            echo "::error::TS_OAUTH_CLIENT_ID is empty"
            exit 1
          fi
          if [ -z "${{ secrets.TS_OAUTH_CLIENT_SECRET }}" ]; then
            echo "::error::TS_OAUTH_CLIENT_SECRET is empty"
            exit 1
          fi      
      # Connect to Tailscale for GitHub-hosted runners
      - name: Connect to Tailscale
        if: needs.check-runner-access.outputs.runner == 'ubuntu-latest'
        uses: tailscale/github-action@v4 # v4
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_CLIENT_SECRET }}
          tags: ${{ inputs.tailscale_tags || 'tag:ci' }}

      - name: Validate inputs
        run: |
          set -e
          
          # Validate app_name (alphanumeric, hyphens, underscores only - max 63 chars for DNS compatibility)
          APP_NAME="${{ inputs.app_name }}"
          if [ -n "$APP_NAME" ]; then
            if [[ ! "$APP_NAME" =~ ^[a-zA-Z][a-zA-Z0-9_-]{0,62}$ ]]; then
              echo "::error::app_name must start with a letter and contain only alphanumeric characters, hyphens, or underscores (max 63 chars)"
              exit 1
            fi
            echo "‚úì app_name validated: $APP_NAME"
          fi
          
          # Validate VLAN tag (only allowed values)
          VLAN_TAG="${{ inputs.vlan_tag }}"
          if [ -n "$VLAN_TAG" ]; then
            if [[ ! "$VLAN_TAG" =~ ^(10|20|30|40|50)$ ]]; then
              echo "::error::vlan_tag must be one of: 10, 20, 30, 40, 50"
              exit 1
            fi
            echo "‚úì vlan_tag validated: $VLAN_TAG"
          fi
          
          # Validate IP address format (if provided)
          TARGET_IP="${{ inputs.vm_target_ip }}"
          if [ -n "$TARGET_IP" ]; then
            if [[ ! "$TARGET_IP" =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]]; then
              echo "::error::IP address format is invalid"
              exit 1
            fi
            echo "‚úì IP address format validated: $TARGET_IP"
          fi
          
          # Validate environment (if provided)
          ENV="${{ inputs.environment }}"
          if [ -n "$ENV" ]; then
            if [[ ! "$ENV" =~ ^(dev|staging|prod|development|production)$ ]]; then
              echo "::error::environment must be one of: dev, staging, prod, development, production"
              exit 1
            fi
            echo "‚úì environment validated: $ENV"
          fi
          
          # Validate resource_type (if provided)
          RESOURCE_TYPE="${{ inputs.resource_type }}"
          if [ -n "$RESOURCE_TYPE" ]; then
            if [[ ! "$RESOURCE_TYPE" =~ ^(vm|lxc)$ ]]; then
              echo "::error::resource_type must be either 'vm' or 'lxc'"
              exit 1
            fi
            echo "‚úì resource_type validated: $RESOURCE_TYPE"
          fi
          
          echo "‚úì All input validation passed"

      - name: Checkout Reusable Workflow Repo
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
        with:
          repository: KoraMaple/nante-reusable-workflow
          ref: develop
          token: ${{ secrets.GH_PAT }}

      - name: Install Doppler CLI
        uses: dopplerhq/cli-action@v3 # v3
      
      # Install Terraform for GitHub-hosted runners (self-hosted runners have it pre-installed)
      - name: Install Terraform
        if: needs.check-runner-access.outputs.runner == 'ubuntu-latest'
        uses: hashicorp/setup-terraform@v3.1.2
        with:
          terraform_wrapper: false

      - name: Configure Doppler
        run: |
          doppler setup --project "${{ secrets.DOPPLER_TARGET_PROJECT }}" --config "${{ secrets.DOPPLER_TARGET_CONFIG }}" --no-interactive
        env:
          DOPPLER_TOKEN: ${{ secrets.DOPPLER_TOKEN }}

      - name: Validate VLAN and IP Match
        run: |
          VLAN="${{ inputs.vlan_tag }}"
          
          # Check if using multi-instance mode
          if [ -n "${{ inputs.instances }}" ]; then
            echo "‚úì Multi-instance mode detected"
            # Validation will happen in Terraform
          else
            IP="${{ inputs.vm_target_ip }}"
            EXPECTED_PREFIX="192.168.${VLAN}."
            
            if [[ ! "$IP" == ${EXPECTED_PREFIX}* ]]; then
              echo "‚ùå Error: IP '$IP' does not match VLAN $VLAN (expected ${EXPECTED_PREFIX}x)"
              exit 1
            fi
            echo "‚úì VLAN $VLAN and IP $IP are valid"
          fi

      - name: Terraform Init, Plan, and Apply
        if: ${{ inputs.skip_terraform != true }}
        run: |
          doppler run -- bash <<'EOF'
          set -e
          
          # Mask all sensitive values to prevent log exposure
          [ -n "$MINIO_ROOT_PASSWORD" ] && echo "::add-mask::$MINIO_ROOT_PASSWORD"
          [ -n "$PROXMOX_TOKEN_SECRET" ] && echo "::add-mask::$PROXMOX_TOKEN_SECRET"
          [ -n "$TAILSCALE_OAUTH_CLIENT_SECRET" ] && echo "::add-mask::$TAILSCALE_OAUTH_CLIENT_SECRET"
          [ -n "$FREEIPA_ADMIN_PASSWORD" ] && echo "::add-mask::$FREEIPA_ADMIN_PASSWORD"
          [ -n "$SSH_PRIVATE_KEY" ] && echo "::add-mask::$SSH_PRIVATE_KEY"
          [ -n "$ANS_SSH_PUBLIC_KEY" ] && echo "::add-mask::$ANS_SSH_PUBLIC_KEY"
          [ -n "$NEXUS_PASSWORD" ] && echo "::add-mask::$NEXUS_PASSWORD"
          [ -n "$OCTOPUS_API_KEY" ] && echo "::add-mask::$OCTOPUS_API_KEY"
          [ -n "$SONAR_TOKEN" ] && echo "::add-mask::$SONAR_TOKEN"
          [ -n "$TS_AUTHKEY" ] && echo "::add-mask::$TS_AUTHKEY"
          [ -n "$TAILSCALE_AUTH_KEY" ] && echo "::add-mask::$TAILSCALE_AUTH_KEY"
          [ -n "$MINIO_ENDPOINT" ] && echo "::add-mask::$MINIO_ENDPOINT"
          
          # Verify credentials are available
          if [ -z "$MINIO_ROOT_USER" ] || [ -z "$MINIO_ROOT_PASSWORD" ]; then
            echo "‚ùå ERROR: MINIO_ROOT_USER or MINIO_ROOT_PASSWORD not found in environment"
            exit 1
          fi
          
          cd terraform/
          
          # Set AWS credentials for Terraform S3 backend
          export AWS_ACCESS_KEY_ID="$MINIO_ROOT_USER"
          export AWS_SECRET_ACCESS_KEY="$MINIO_ROOT_PASSWORD"
          
          # Use MinIO endpoint from Doppler (required secret)
          if [ -z "$MINIO_ENDPOINT" ]; then
            echo "‚ùå ERROR: MINIO_ENDPOINT not found in Doppler. Please configure this secret."
            exit 1
          fi
          
          # Terraform Init
          terraform init \
            -backend-config="bucket=terraform-state" \
            -backend-config="endpoints={s3=\"${MINIO_ENDPOINT}\"}" \
            -backend-config="access_key=$MINIO_ROOT_USER" \
            -backend-config="secret_key=$MINIO_ROOT_PASSWORD"
          
          # Select workspace
          terraform workspace select ${{ inputs.app_name }} || terraform workspace new ${{ inputs.app_name }}
          
          # Check if VM already exists (for retry scenarios)
          if terraform state list | grep -q "proxmox_vm_qemu.generic_vm" || terraform state list | grep -q "proxmox_lxc.container"; then
            echo "‚ö†Ô∏è  Resources already exist in Terraform state"
            echo "This is likely a retry after a previous failure or reconfiguration of existing VMs"
            echo "Skipping Terraform apply, proceeding to Ansible configuration..."
            
            # Extract outputs even though we're not running apply
            # This is critical for getting hostnames and Tailscale auth key
            echo "Extracting Terraform outputs for existing resources..."
            
            # We're already in terraform directory from line 171
            # Try to get Tailscale auth key from Terraform output first
            TAILSCALE_AUTH_KEY=$(terraform output -raw tailscale_auth_key 2>/dev/null || echo "")
            
            # If no key in Terraform, generate one via OAuth
            if [ -z "$TAILSCALE_AUTH_KEY" ]; then
              echo "No Tailscale auth key in state, generating via OAuth..."
              mkdir -p /tmp/tailscale-provision-$$
              cd /tmp/tailscale-provision-$$
              
              cat > main.tf <<'TSEOF'
          terraform {
            required_providers {
              tailscale = {
                source  = "tailscale/tailscale"
                version = "~> 0.17"
              }
            }
          }
          
          provider "tailscale" {}
          
          resource "tailscale_tailnet_key" "provision_key" {
            reusable      = true
            ephemeral     = false
            preauthorized = true
            expiry        = 7776000
            description   = "Provision key for ${{ inputs.app_name }}"
          }
          
          output "auth_key" {
            value     = tailscale_tailnet_key.provision_key.key
            sensitive = true
          }
          TSEOF
              
              terraform init -no-color > /dev/null 2>&1
              if terraform apply -auto-approve -no-color > /dev/null 2>&1; then
                TAILSCALE_AUTH_KEY=$(terraform output -raw auth_key 2>/dev/null || echo "")
                echo "‚úì Generated Tailscale auth key via OAuth"
              fi
              cd -
              rm -rf /tmp/tailscale-provision-$$
            fi
            
            # Export auth key (masked for security)
            if [ -n "$TAILSCALE_AUTH_KEY" ]; then
              echo "::add-mask::$TAILSCALE_AUTH_KEY"
              echo "TAILSCALE_AUTH_KEY=$TAILSCALE_AUTH_KEY" >> $GITHUB_ENV
              echo "‚úì Tailscale auth key available"
            fi
            
            # Return to terraform directory if we went to temp dir for Tailscale
            cd $GITHUB_WORKSPACE/terraform/
            
            # Get VM/LXC hostnames from Terraform outputs
            echo "Extracting hostname for resource_type=${{ inputs.resource_type }}"
            if [ -n "${{ inputs.instances }}" ]; then
              VM_HOSTNAMES_JSON=$(terraform output -json vm_hostnames 2>/dev/null || echo '{}')
              echo "VM_HOSTNAMES_JSON=$VM_HOSTNAMES_JSON" >> $GITHUB_ENV
              echo "‚úì Multi-instance hostnames retrieved"
            else
              # Single-instance mode - check for VM or LXC hostname
              if [ "${{ inputs.resource_type }}" = "lxc" ]; then
                echo "Extracting lxc_hostname..."
                VM_HOSTNAME=$(terraform output -raw lxc_hostname 2>/dev/null || echo "")
              else
                echo "Extracting vm_hostname..."
                VM_HOSTNAME=$(terraform output -raw vm_hostname 2>/dev/null || echo "")
              fi
              if [ -n "$VM_HOSTNAME" ]; then
                echo "VM_HOSTNAME=$VM_HOSTNAME" >> $GITHUB_ENV
                echo "‚úì Hostname: $VM_HOSTNAME"
              else
                echo "‚ö†Ô∏è  WARNING: Hostname is empty after extraction"
              fi
            fi
            
            # Skip to Ansible
            exit 0
          fi
          
          # Set Proxmox credentials
          export TF_VAR_proxmox_api_url="$PROXMOX_API_URL"
          export TF_VAR_proxmox_api_token_id="$PROXMOX_TOKEN_ID"
          export TF_VAR_proxmox_api_token_secret="$PROXMOX_TOKEN_SECRET"
          export TF_VAR_ssh_public_key="$ANS_SSH_PUBLIC_KEY"
          
          # Set Tailscale credentials for Terraform provider
          export TAILSCALE_OAUTH_CLIENT_ID="$TAILSCALE_OAUTH_CLIENT_ID"
          export TAILSCALE_OAUTH_CLIENT_SECRET="$TAILSCALE_OAUTH_CLIENT_SECRET"
          export TAILSCALE_TAILNET="${TAILSCALE_TAILNET:--}"
          
          # Create Tailscale cloud-init snippet
          TAILSCALE_AUTH_KEY=$(terraform output -raw tailscale_auth_key 2>/dev/null || echo "")
          if [ -z "$TAILSCALE_AUTH_KEY" ]; then
            # First run - need to create the key
            echo "Creating Tailscale auth key..."
          fi
          
          # Generate cloud-init snippet for Tailscale
          cat > /tmp/${{ inputs.app_name }}-tailscale.yml <<CLOUDINIT
          #cloud-config
          runcmd:
            - curl -fsSL https://tailscale.com/install.sh | sh
            - tailscale up --authkey=\${TAILSCALE_AUTH_KEY} --hostname=${{ inputs.app_name }} --accept-routes
            - timeout 30 sh -c 'until tailscale status --json | grep -q "\"Online\":true"; do sleep 2; done' || true
          CLOUDINIT
          
          # Upload to Proxmox snippets directory (requires SSH access or NFS mount)
          # For now, we'll note this in the output
          echo "üìù Cloud-init snippet created at /tmp/${{ inputs.app_name }}-tailscale.yml"
          echo "Note: Upload to Proxmox at /var/lib/vz/snippets/${{ inputs.app_name }}-tailscale.yml"
          
          # Terraform Plan
          PLAN_ARGS=(
            -var="app_name=${{ inputs.app_name }}"
            -var="environment=${{ inputs.environment }}"
            -var="vlan_tag=${{ inputs.vlan_tag }}"
            -var="vm_cpu_cores=${{ inputs.cpu_cores }}"
            -var="vm_ram_mb=${{ inputs.ram_mb }}"
            -var="vm_disk_gb=${{ inputs.disk_gb }}"
            -var="proxmox_target_node=${{ inputs.proxmox_node }}"
            -var="proxmox_storage=${{ inputs.proxmox_storage }}"
            -var="vm_template=${{ inputs.vm_template }}"
            -var="resource_type=${{ inputs.resource_type }}"
            -var="lxc_template=${{ inputs.lxc_template }}"
            -var="lxc_nesting=${{ inputs.lxc_nesting }}"
            -var="lxc_unprivileged=${{ inputs.lxc_unprivileged }}"
            -var="enable_tailscale_terraform=false"
          )
          
          # Add instances or vm_target_ip based on mode
          if [ -n "${{ inputs.instances }}" ]; then
            echo "Using multi-instance mode"
            echo '${{ inputs.instances }}' > /tmp/instances.json
            PLAN_ARGS+=(-var="instances=$(cat /tmp/instances.json)")
          else
            echo "Using single-instance mode"
            PLAN_ARGS+=(-var="vm_target_ip=${{ inputs.vm_target_ip }}")
          fi
          
          terraform plan "${PLAN_ARGS[@]}" -out=tfplan
          
          # Terraform Apply
          if ! terraform apply tfplan; then
            echo "‚ùå Terraform apply failed"
            echo "If VM was partially created, you can:"
            echo "1. Re-run this workflow (it will detect existing VM and skip to Ansible)"
            echo "2. Or manually destroy: cd terraform && terraform destroy"
            exit 1
          fi
          
          # Configure TUN device for LXC containers (required for Tailscale)
          if [ "${{ inputs.resource_type }}" = "lxc" ]; then
            echo "‚ö†Ô∏è  MANUAL ACTION REQUIRED: TUN device must be configured for LXC containers"
            LXC_ID=$(terraform output -raw lxc_id 2>/dev/null)
            if [ -n "$LXC_ID" ]; then
              PROXMOX_NODE="${{ inputs.proxmox_node }}"
              echo ""
              echo "=========================================="
              echo "LXC Container Created: ${LXC_ID}"
              echo "Proxmox Node: ${PROXMOX_NODE}"
              echo "=========================================="
              echo ""
              echo "To enable Tailscale, run these commands on the Proxmox host:"
              echo ""
              echo "  pct stop ${LXC_ID}"
              printf "  echo 'lxc.cgroup2.%s.allow: c 10:200 rwm' >> /etc/pve/lxc/${LXC_ID}.conf\n" "devices"
              printf "  echo 'lxc.mount.entry: /%s/net/tun %s/net/tun none bind,create=file' >> /etc/pve/lxc/${LXC_ID}.conf\n" "dev" "dev"
              echo "  pct start ${LXC_ID}"
              echo ""
              echo "=========================================="
              echo ""
              echo "‚è≥ Waiting 3 minutes for you to configure TUN device..."
              echo "   (Workflow will continue automatically after 180 seconds)"
              echo ""
              
              # Wait without countdown to keep commands visible
              sleep 180
              
              echo ""
              echo "‚úì Continuing with workflow..."
              echo "   Assuming TUN device has been configured."
            else
              echo "‚ö†Ô∏è  Could not get LXC ID from Terraform output"
            fi
          fi
          
          # Get Tailscale auth key for Ansible (masked for security)
          export TAILSCALE_AUTH_KEY=$(terraform output -raw tailscale_auth_key 2>/dev/null || echo "")
          if [ -n "$TAILSCALE_AUTH_KEY" ]; then
            echo "::add-mask::$TAILSCALE_AUTH_KEY"
            echo "TAILSCALE_AUTH_KEY=$TAILSCALE_AUTH_KEY" >> $GITHUB_ENV
            echo "‚úì Tailscale auth key generated by Terraform"
          else
            echo "‚ö†Ô∏è  No Tailscale auth key from Terraform - generating directly via OAuth..."
            
            # Generate auth key directly using Tailscale OAuth credentials
            mkdir -p /tmp/tailscale-provision-$$
            cd /tmp/tailscale-provision-$$
            
            cat > main.tf <<'TSEOF'
          terraform {
            required_providers {
              tailscale = {
                source  = "tailscale/tailscale"
                version = "~> 0.17"
              }
            }
          }
          
          provider "tailscale" {
            # Uses environment variables:
            # TAILSCALE_OAUTH_CLIENT_ID
            # TAILSCALE_OAUTH_CLIENT_SECRET
            # TAILSCALE_TAILNET
          }
          
          resource "tailscale_tailnet_key" "provision_key" {
            reusable      = true
            ephemeral     = false
            preauthorized = true
            expiry        = 7776000  # 90 days
            description   = "Provision key for ${{ inputs.app_name }}"
          }
          
          output "auth_key" {
            value     = tailscale_tailnet_key.provision_key.key
            sensitive = true
          }
          TSEOF
            
            # Initialize and apply
            terraform init -no-color > /dev/null 2>&1
            if terraform apply -auto-approve -no-color > /dev/null 2>&1; then
              TAILSCALE_AUTH_KEY=$(terraform output -raw auth_key 2>/dev/null || echo "")
              if [ -n "$TAILSCALE_AUTH_KEY" ]; then
                echo "::add-mask::$TAILSCALE_AUTH_KEY"
                echo "TAILSCALE_AUTH_KEY=$TAILSCALE_AUTH_KEY" >> $GITHUB_ENV
                echo "‚úì Tailscale auth key generated via OAuth"
              else
                echo "‚ö†Ô∏è  Failed to extract Tailscale auth key"
              fi
            else
              echo "‚ö†Ô∏è  Failed to generate Tailscale auth key - check OAuth credentials in Doppler"
            fi
            
            # Cleanup
            cd /
            rm -rf /tmp/tailscale-provision-$$
          fi
          
          # Get VM hostnames from Terraform for Ansible
          # In multi-instance mode, this is a JSON map; in single-instance mode, it's a single hostname
          if [ -n "${{ inputs.instances }}" ]; then
            # Multi-instance mode - get all hostnames and IPs as JSON
            VM_HOSTNAMES_JSON=$(terraform output -json vm_hostnames 2>/dev/null || echo '{}')
            VM_IPS_JSON=$(terraform output -json vm_target_ips 2>/dev/null || echo '{}')
            echo "VM_HOSTNAMES_JSON=$VM_HOSTNAMES_JSON" >> $GITHUB_ENV
            echo "VM_IPS_JSON=$VM_IPS_JSON" >> $GITHUB_ENV
            echo "‚úì Multi-instance hostnames retrieved"
            echo "$VM_HOSTNAMES_JSON" | jq -r 'to_entries[] | "  \(.key): \(.value)"'
          else
            # Single-instance mode - get single hostname (VM or LXC)
            if [ "${{ inputs.resource_type }}" = "lxc" ]; then
              VM_HOSTNAME=$(terraform output -raw lxc_hostname 2>/dev/null || echo "")
            else
              # For VMs, extract from vm_hostnames JSON map (first entry)
              VM_HOSTNAME=$(terraform output -json vm_hostnames 2>/dev/null | jq -r 'to_entries[0].value' 2>/dev/null || echo "")
            fi
            if [ -n "$VM_HOSTNAME" ]; then
              echo "VM_HOSTNAME=$VM_HOSTNAME" >> $GITHUB_ENV
              echo "‚úì Hostname: $VM_HOSTNAME"
            else
              echo "‚ö†Ô∏è  No hostname found in Terraform output"
            fi
          fi
          EOF

      - name: Wait for VM to boot and SSH to be ready
        run: |
          # For GitHub-hosted runners, wait for Tailscale connectivity
          # For self-hosted runners, wait for local IP connectivity
          RUNNER_TYPE="${{ needs.check-runner-access.outputs.runner }}"
          
          if [ "$RUNNER_TYPE" = "ubuntu-latest" ]; then
            echo "GitHub-hosted runner detected - will wait for Tailscale connectivity"
            
            # Get hostnames from environment
            if [ -n "${{ inputs.instances }}" ]; then
              # Multi-instance mode
              HOSTNAMES=$(echo '${{ env.VM_HOSTNAMES_JSON }}' | jq -r '.[] | select(. != null and . != "")' | tr '\n' ' ')
              echo "Multi-instance mode: waiting for Tailscale hostnames: $HOSTNAMES"
            else
              # Single-instance mode
              HOSTNAMES="${{ env.VM_HOSTNAME }}"
              echo "Single-instance mode: waiting for Tailscale hostname: $HOSTNAMES"
            fi
            
            if [ -z "$HOSTNAMES" ] || [ "$HOSTNAMES" = "null" ]; then
              echo "‚ùå ERROR: No Tailscale hostnames available"
              echo "VM_HOSTNAME: ${{ env.VM_HOSTNAME }}"
              echo "VM_HOSTNAMES_JSON: ${{ env.VM_HOSTNAMES_JSON }}"
              exit 1
            fi
            
            # Wait for each hostname to be reachable via Tailscale
            for HOSTNAME in $HOSTNAMES; do
              echo "Waiting for ${{ inputs.resource_type }} at $HOSTNAME to join Tailscale..."
              
              # Wait up to 5 minutes for Tailscale connectivity
              TAILSCALE_SUCCESS=false
              MAX_ATTEMPTS=60
              
              for i in $(seq 1 $MAX_ATTEMPTS); do
                if ping -c 1 -W 2 "$HOSTNAME" &>/dev/null; then
                  echo "‚úì $HOSTNAME is reachable via Tailscale (attempt $i)"
                  TAILSCALE_SUCCESS=true
                  break
                fi
                echo "Attempt $i/$MAX_ATTEMPTS: $HOSTNAME not yet on Tailscale, waiting 5s..."
                sleep 5
              done
              
              if [ "$TAILSCALE_SUCCESS" = false ]; then
                echo "‚ùå ERROR: $HOSTNAME did not join Tailscale after 5 minutes"
                echo "Check that:"
                echo "  1. VM/CT was created successfully in Proxmox"
                echo "  2. Tailscale installation completed (check cloud-init logs)"
                echo "  3. TAILSCALE_AUTH_KEY is valid"
                echo "  4. VM/CT has internet connectivity"
                exit 1
              fi
              
              # Wait for SSH on Tailscale hostname
              echo "Waiting for SSH on $HOSTNAME..."
              SSH_SUCCESS=false
              MAX_ATTEMPTS=24
              
              for i in $(seq 1 $MAX_ATTEMPTS); do
                if nc -z -w 2 "$HOSTNAME" 22 2>/dev/null; then
                  echo "‚úì SSH port at $HOSTNAME is open (attempt $i)"
                  SSH_SUCCESS=true
                  break
                fi
                echo "Attempt $i/$MAX_ATTEMPTS: SSH at $HOSTNAME not ready, waiting 5s..."
                sleep 5
              done
              
              if [ "$SSH_SUCCESS" = false ]; then
                echo "‚ùå ERROR: SSH port at $HOSTNAME did not open after 2 minutes"
                exit 1
              fi
              
              echo "‚úì $HOSTNAME is ready for Ansible configuration"
            done
          else
            echo "Self-hosted runner detected - will use local IP connectivity"
            
            # Determine target IPs based on deployment mode
            if [ -n "${{ inputs.instances }}" ]; then
              # Multi-instance mode - extract IPs from instances JSON
              TARGET_IPS=$(echo '${{ inputs.instances }}' | jq -r '.[] | .ip_address' | tr '\n' ' ')
              echo "Multi-instance mode: waiting for instances at: $TARGET_IPS"
            else
              # Single-instance mode
              TARGET_IPS="${{ inputs.vm_target_ip }}"
              echo "Single-instance mode: waiting for ${{ inputs.resource_type }} at $TARGET_IPS"
            fi
            
            # Wait for each IP
            for TARGET_IP in $TARGET_IPS; do
              echo "Waiting for ${{ inputs.resource_type }} at $TARGET_IP to be reachable..."
          
          # Wait up to 3 minutes for the resource to respond to ping
          PING_SUCCESS=false
          MAX_ATTEMPTS=36
          # LXC containers boot much faster, reduce wait time
          if [ "${{ inputs.resource_type }}" = "lxc" ]; then
            MAX_ATTEMPTS=12  # 1 minute for LXC
          fi
          
            for i in $(seq 1 $MAX_ATTEMPTS); do
              if ping -c 1 -W 2 $TARGET_IP &>/dev/null; then
                echo "‚úì ${{ inputs.resource_type }} at $TARGET_IP is responding to ping (attempt $i)"
                PING_SUCCESS=true
                break
              fi
              echo "Attempt $i/$MAX_ATTEMPTS: ${{ inputs.resource_type }} at $TARGET_IP not yet reachable, waiting 5s..."
              sleep 5
            done
          
            if [ "$PING_SUCCESS" = false ]; then
              echo "‚ùå ERROR: ${{ inputs.resource_type }} at $TARGET_IP did not respond to ping"
              echo "Check that:"
              echo "  1. Resource was created successfully in Proxmox"
              if [ "${{ inputs.resource_type }}" = "vm" ]; then
                echo "  2. Cloud-init completed (check VM console)"
              fi
              echo "  3. Network configuration is correct (VLAN ${{ inputs.vlan_tag }})"
              echo "  4. IP $TARGET_IP is not already in use"
              exit 1
            fi
          
          # Wait for SSH port to be open
          # LXC containers (especially RHEL-based) may take longer to start SSH
          if [ "${{ inputs.resource_type }}" = "lxc" ]; then
            MAX_ATTEMPTS=36  # 3 minutes for LXC
            echo "Waiting for SSH port 22 (LXC may take up to 3 minutes)..."
          else
            MAX_ATTEMPTS=24  # 2 minutes for VMs
            echo "Waiting for SSH port 22..."
          fi
          
            SSH_SUCCESS=false
            for i in $(seq 1 $MAX_ATTEMPTS); do
              if nc -z -w 2 $TARGET_IP 22 2>/dev/null; then
                echo "‚úì SSH port at $TARGET_IP is open (attempt $i)"
                SSH_SUCCESS=true
                break
              fi
              echo "Attempt $i/$MAX_ATTEMPTS: SSH at $TARGET_IP not ready, waiting 5s..."
              sleep 5
            done
          
            if [ "$SSH_SUCCESS" = false ]; then
              if [ "${{ inputs.resource_type }}" = "lxc" ]; then
                echo "‚ùå ERROR: SSH port at $TARGET_IP did not open after 3 minutes"
                echo "Check that:"
                echo "  1. LXC container started successfully in Proxmox"
                echo "  2. SSH service is installed and enabled (systemctl status sshd)"
                echo "  3. Network configuration is correct (VLAN ${{ inputs.vlan_tag }})"
                echo "  4. Firewall is not blocking port 22"
              else
                echo "‚ùå ERROR: SSH port at $TARGET_IP did not open after 2 minutes"
                echo "Check that:"
                echo "  1. Cloud-init completed successfully"
                echo "  2. SSH service is running"
                echo "  3. Firewall is not blocking port 22"
              fi
              exit 1
            fi
            
            # Additional wait for cloud-init to fully complete (VMs only)
            if [ "${{ inputs.resource_type }}" = "vm" ]; then
              echo "Waiting 15s for cloud-init to complete on $TARGET_IP..."
              sleep 15
              echo "‚úì VM at $TARGET_IP is ready for Ansible configuration"
            else
              echo "‚úì LXC container at $TARGET_IP is ready for Ansible configuration"
            fi
          done

      - name: Run Ansible with Doppler
        env:
          TAILSCALE_AUTH_KEY: ${{ env.TAILSCALE_AUTH_KEY }}
        run: |
          doppler run -- bash <<'EOF'
          set -e
          
          # Mask sensitive values to prevent log exposure
          if [ -n "$SSH_PRIVATE_KEY" ]; then echo "::add-mask::$SSH_PRIVATE_KEY"; fi
          if [ -n "$FREEIPA_ADMIN_PASSWORD" ]; then echo "::add-mask::$FREEIPA_ADMIN_PASSWORD"; fi
          if [ -n "$TAILSCALE_AUTH_KEY" ]; then echo "::add-mask::$TAILSCALE_AUTH_KEY"; fi
          
          # Setup SSH agent with key from Doppler
          eval $(ssh-agent -s)
          echo "$SSH_PRIVATE_KEY" | ssh-add -
          
          cd ansible/
          
          # Install requirements
          ansible-galaxy install -r requirements.yml
          
          # Determine Ansible inventory based on runner type
          RUNNER_TYPE="${{ needs.check-runner-access.outputs.runner }}"
          
          if [ "$RUNNER_TYPE" = "ubuntu-latest" ]; then
            echo "GitHub-hosted runner - using Tailscale hostnames for Ansible"
            
            # Use Tailscale hostnames instead of IPs
            if [ -n "${{ inputs.instances }}" ]; then
              # Multi-instance mode - build inventory from hostnames
              HOSTNAMES=$(echo '${{ env.VM_HOSTNAMES_JSON }}' | jq -r '.[] | select(. != null and . != "")' | tr '\n' ',')
              INVENTORY="${HOSTNAMES}"
              echo "Multi-instance inventory (Tailscale): $INVENTORY"
            else
              # Single-instance mode
              INVENTORY="${{ env.VM_HOSTNAME }},"
              echo "Single-instance inventory (Tailscale): $INVENTORY"
            fi
            
            # Clean up old SSH host keys for Tailscale hostnames
            for HOST in $(echo "$INVENTORY" | tr ',' ' '); do
              [ -n "$HOST" ] && ssh-keygen -R "$HOST" 2>/dev/null || true
            done
          else
            echo "Self-hosted runner - using local IP addresses for Ansible"
            
            # Use local IPs
            if [ -n "${{ inputs.instances }}" ]; then
              # Multi-instance mode - build inventory from instances
              TARGET_IPS=$(echo '${{ inputs.instances }}' | jq -r '.[] | .ip_address' | tr '\n' ',')
              INVENTORY="${TARGET_IPS}"
              echo "Multi-instance inventory (local): $INVENTORY"
            else
              # Single-instance mode
              INVENTORY="${{ inputs.vm_target_ip }},"
              echo "Single-instance inventory (local): $INVENTORY"
            fi
            
            # Clean up old SSH host keys for IPs
            for IP in $(echo "$INVENTORY" | tr ',' ' '); do
              [ -n "$IP" ] && ssh-keygen -R "$IP" 2>/dev/null || true
            done
          fi
          
          # Determine Ansible user based on resource type
          # VMs use cloud-init to create 'deploy' user
          # LXC containers only support SSH keys for root, so we run as root
          ANSIBLE_USER="deploy"
          if [ "${{ inputs.resource_type }}" = "lxc" ]; then
            echo "LXC detected - will run Ansible as root"
            ANSIBLE_USER="root"
          fi
          
          # Test connectivity
          echo "Testing Ansible connectivity as $ANSIBLE_USER..."
          if ! ansible all -i "$INVENTORY" -m ping \
            --user $ANSIBLE_USER \
            --ssh-common-args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10'; then
            echo "‚ùå ERROR: Ansible cannot connect to ${{ inputs.resource_type }}"
            exit 1
          fi
          echo "‚úì Ansible connectivity confirmed for all hosts"
          
          # Parse Octopus roles from comma-separated string to JSON array
          OCTOPUS_ROLES_JSON="[]"
          if [ -n "${{ inputs.octopus_roles }}" ]; then
            # Simple bash parsing - no jq needed
            IFS=',' read -ra ROLES <<< "${{ inputs.octopus_roles }}"
            OCTOPUS_ROLES_JSON="["
            for i in "${!ROLES[@]}"; do
              ROLE=$(echo "${ROLES[$i]}" | xargs)  # trim whitespace
              if [ -n "$ROLE" ]; then
                [ "$i" -gt 0 ] && OCTOPUS_ROLES_JSON+=","
                OCTOPUS_ROLES_JSON+="\"$ROLE\""
              fi
            done
            OCTOPUS_ROLES_JSON+="]"
          fi
          
          # Run playbook with all integrations (Tailscale, LDAP, Octopus, Grafana)
          # Note: LXC runs as root (no become needed), VMs run as deploy user (needs become)
          BECOME_FLAG=""
          if [ "${{ inputs.resource_type }}" = "vm" ]; then
            BECOME_FLAG="--become"
          fi
          
          echo "Running Ansible playbook as $ANSIBLE_USER..."
          
          # Set TS_AUTHKEY for Ansible from environment (passed from previous step)
          # Mask the auth key to prevent accidental exposure in logs
          if [ -n "$TAILSCALE_AUTH_KEY" ]; then
            echo "::add-mask::$TAILSCALE_AUTH_KEY"
          fi
          
          if [ -z "$TAILSCALE_AUTH_KEY" ]; then
            echo "‚ö†Ô∏è  WARNING: No Tailscale auth key available. Tailscale will not be configured."
            echo "   Check that TAILSCALE_OAUTH_CLIENT_ID and TAILSCALE_OAUTH_CLIENT_SECRET are set in Doppler"
            export TS_AUTHKEY=""
          else
            export TS_AUTHKEY="$TAILSCALE_AUTH_KEY"
            echo "‚úì Using Tailscale auth key for configuration"
          fi
          
          # Get VM hostnames from environment (set in previous step)
          if [ -n "${{ inputs.instances }}" ]; then
            # Multi-instance mode - hostnames are in JSON map
            VM_HOSTNAMES_JSON='${{ env.VM_HOSTNAMES_JSON }}'
            echo "‚úì Using multi-instance hostnames from Terraform"
          else
            # Single-instance mode
            VM_HOSTNAME="${{ env.VM_HOSTNAME }}"
            if [ -z "$VM_HOSTNAME" ]; then
              echo "‚ö†Ô∏è  WARNING: VM_HOSTNAME not found in environment"
            else
              echo "‚úì Using VM hostname: $VM_HOSTNAME"
            fi
          fi
          
          # FreeIPA LDAP configuration (from Doppler)
          # These are used by ldap-config role to enroll VMs/CTs with FreeIPA
          if [ -n "$FREEIPA_SERVER_IP" ] && [ -n "$FREEIPA_ADMIN_PASSWORD" ]; then
            echo "‚úì FreeIPA LDAP configuration available - will configure LDAP client"
          else
            echo "‚ö†Ô∏è  FreeIPA credentials not found in Doppler - LDAP client will not be configured"
          fi
          
          # Parse ansible_roles from comma-separated string to JSON array
          ANSIBLE_ROLES_JSON="[]"
          if [ -n "${{ inputs.ansible_roles }}" ]; then
            IFS=',' read -ra ROLES <<< "${{ inputs.ansible_roles }}"
            ANSIBLE_ROLES_JSON="["
            for i in "${!ROLES[@]}"; do
              ROLE=$(echo "${ROLES[$i]}" | xargs)  # trim whitespace
              if [ -n "$ROLE" ]; then
                [ "$i" -gt 0 ] && ANSIBLE_ROLES_JSON+=","
                ANSIBLE_ROLES_JSON+="\"$ROLE\""
              fi
            done
            ANSIBLE_ROLES_JSON+="]"
          fi
          
          echo "Ansible roles to apply: $ANSIBLE_ROLES_JSON"
          
          # Parse ansible_extra_vars if provided
          if [ -n "${{ inputs.ansible_extra_vars }}" ]; then
            echo "‚úì Additional Ansible extra vars provided"
          fi
          
          # Run Ansible playbook
          # In multi-instance mode, Ansible will run against all hosts in inventory
          # Each host will get its hostname from hostvars or we'll use a generic pattern
          # Export TS_AUTHKEY so it's available to ansible-playbook subprocess
          export TS_AUTHKEY
          
          if [ -n "${{ inputs.instances }}" ]; then
            # Multi-instance: pass hostnames as JSON for roles to use
            if [ -n "${{ inputs.ansible_extra_vars }}" ]; then
              TS_AUTHKEY="$TS_AUTHKEY" ansible-playbook -i "$INVENTORY" site.yml \
                --user $ANSIBLE_USER \
                $BECOME_FLAG \
                --ssh-common-args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' \
                --extra-vars "vlan_tag=${{ inputs.vlan_tag }}" \
                --extra-vars "ansible_roles=$ANSIBLE_ROLES_JSON" \
                --extra-vars "vm_hostnames_json=$VM_HOSTNAMES_JSON" \
                --extra-vars "octopus_environment=${{ inputs.octopus_environment }}" \
                --extra-vars "octopus_roles=$OCTOPUS_ROLES_JSON" \
                --extra-vars "skip_octopus=${{ inputs.skip_octopus }}" \
                --extra-vars "freeipa_server_ip=${FREEIPA_SERVER_IP:-}" \
                --extra-vars "freeipa_admin_password=${FREEIPA_ADMIN_PASSWORD:-}" \
                --extra-vars "${{ inputs.ansible_extra_vars }}"
            else
              TS_AUTHKEY="$TS_AUTHKEY" ansible-playbook -i "$INVENTORY" site.yml \
                --user $ANSIBLE_USER \
                $BECOME_FLAG \
                --ssh-common-args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' \
                --extra-vars "vlan_tag=${{ inputs.vlan_tag }}" \
                --extra-vars "ansible_roles=$ANSIBLE_ROLES_JSON" \
                --extra-vars "vm_hostnames_json=$VM_HOSTNAMES_JSON" \
                --extra-vars "octopus_environment=${{ inputs.octopus_environment }}" \
                --extra-vars "octopus_roles=$OCTOPUS_ROLES_JSON" \
                --extra-vars "skip_octopus=${{ inputs.skip_octopus }}" \
                --extra-vars "freeipa_server_ip=${FREEIPA_SERVER_IP:-}" \
                --extra-vars "freeipa_admin_password=${FREEIPA_ADMIN_PASSWORD:-}"
            fi
          else
            # Single-instance: pass single hostname
            if [ -n "${{ inputs.ansible_extra_vars }}" ]; then
              echo "Running ansible-playbook with extra vars..."
              TS_AUTHKEY="$TS_AUTHKEY" ansible-playbook -i "$INVENTORY" site.yml \
                --user $ANSIBLE_USER \
                $BECOME_FLAG \
                --ssh-common-args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' \
                --extra-vars "target_hostname=$VM_HOSTNAME" \
                --extra-vars "vlan_tag=${{ inputs.vlan_tag }}" \
                --extra-vars "ansible_roles=$ANSIBLE_ROLES_JSON" \
                --extra-vars "octopus_environment=${{ inputs.octopus_environment }}" \
                --extra-vars "octopus_roles=$OCTOPUS_ROLES_JSON" \
                --extra-vars "skip_octopus=${{ inputs.skip_octopus }}" \
                --extra-vars "freeipa_server_ip=${FREEIPA_SERVER_IP:-}" \
                --extra-vars "freeipa_admin_password=${FREEIPA_ADMIN_PASSWORD:-}" \
                --extra-vars "${{ inputs.ansible_extra_vars }}"
            else
              TS_AUTHKEY="$TS_AUTHKEY" ansible-playbook -i "$INVENTORY" site.yml \
                --user $ANSIBLE_USER \
                $BECOME_FLAG \
                --ssh-common-args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' \
                --extra-vars "target_hostname=$VM_HOSTNAME" \
                --extra-vars "vlan_tag=${{ inputs.vlan_tag }}" \
                --extra-vars "ansible_roles=$ANSIBLE_ROLES_JSON" \
                --extra-vars "octopus_environment=${{ inputs.octopus_environment }}" \
                --extra-vars "octopus_roles=$OCTOPUS_ROLES_JSON" \
                --extra-vars "skip_octopus=${{ inputs.skip_octopus }}" \
                --extra-vars "freeipa_server_ip=${FREEIPA_SERVER_IP:-}" \
                --extra-vars "freeipa_admin_password=${FREEIPA_ADMIN_PASSWORD:-}"
            fi
          fi
          
          echo "‚úì VM/LXC provisioned and configured"
          if [ -n "${{ inputs.ansible_roles }}" ]; then
            echo "‚úì Applied roles: ${{ inputs.ansible_roles }}"
          fi
          echo "‚úì Octopus Tentacle installed and registered"
          echo ""
          echo "üìã Next steps:"
          echo "  1. Check Tailscale admin console - device should appear within 1-2 minutes"
          echo "  2. Check Octopus Deploy - target should be healthy"
          echo "  3. SSH to VM: ssh deploy@${{ inputs.vm_target_ip }}"
          EOF