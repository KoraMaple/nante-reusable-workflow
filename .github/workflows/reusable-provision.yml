name: "Reusable VM/CT Provisioning"

on:
  workflow_call:
    inputs:
      app_name:
        type: string
        description: 'Application Name (e.g. nexus, k3s, octopus)'
        required: true
        default: 'app'
      vlan_tag:
        type: string
        description: 'Network Zone'
        default: '20'
      vm_target_ip:
        type: string
        description: 'Target IP Address'
        required: true
      cpu_cores:
        type: string
        description: 'CPU Cores'
        required: true
        default: '2'
      ram_mb:
        type: string
        description: 'RAM in MB'
        required: true
        default: '4096'
      disk_gb:
        type: string
        description: 'Disk Size (e.g. 20G)'
        required: true
        default: '20G'
      # Optional infrastructure overrides
      proxmox_node:
        type: string
        description: 'Proxmox node to deploy on'
        required: false
        default: 'pmx'
      proxmox_storage:
        type: string
        description: 'Storage pool for VM disks'
        required: false
        default: 'zfs-vm'
      vm_template:
        type: string
        description: 'VM template to clone'
        required: false
        default: 'ubuntu-2404-template'
      environment:
        type: string
        description: 'Environment name (dev, staging, prod)'
        required: false
        default: 'dev'
      # Resource type selection
      resource_type:
        type: string
        description: 'Resource type: vm or lxc'
        required: false
        default: 'vm'
      lxc_template:
        type: string
        description: 'LXC template (only for resource_type=lxc)'
        required: false
        default: 'local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.zst'
      lxc_nesting:
        type: boolean
        description: 'Enable nesting for Docker in LXC'
        required: false
        default: false
      lxc_unprivileged:
        type: boolean
        description: 'Run LXC as unprivileged (recommended for security)'
        required: false
        default: true
      # Skip Terraform for existing VMs
      skip_terraform:
        type: boolean
        description: 'Skip Terraform provisioning (use existing VM)'
        required: false
        default: false
      # Octopus Deploy configuration
      octopus_environment:
        type: string
        description: 'Octopus Deploy environment (Development, Staging, Production)'
        required: false
        default: 'Development'
      octopus_roles:
        type: string
        description: 'Comma-separated Octopus roles (e.g., web-server,nginx)'
        required: false
        default: ''
      skip_octopus:
        type: boolean
        description: 'Skip Octopus Tentacle registration'
        required: false
        default: false
    secrets:
      DOPPLER_TOKEN:
        required: true
      DOPPLER_TARGET_PROJECT:
        required: true
      DOPPLER_TARGET_CONFIG:
        required: true
      GH_PAT:
        required: true

jobs:
  provision:
    runs-on: self-hosted
    
    steps:
      - name: Checkout Reusable Workflow Repo
        uses: actions/checkout@v4
        with:
          repository: KoraMaple/nante-reusable-workflow
          ref: develop
          token: ${{ secrets.GH_PAT }}

      - name: Install Doppler CLI
        uses: dopplerhq/cli-action@v3
      
      - name: Configure Doppler
        run: |
          doppler setup --project "${{ secrets.DOPPLER_TARGET_PROJECT }}" --config "${{ secrets.DOPPLER_TARGET_CONFIG }}" --no-interactive
        env:
          DOPPLER_TOKEN: ${{ secrets.DOPPLER_TOKEN }}

      - name: Validate VLAN and IP Match
        run: |
          VLAN="${{ inputs.vlan_tag }}"
          IP="${{ inputs.vm_target_ip }}"
          EXPECTED_PREFIX="192.168.${VLAN}."
          
          if [[ ! "$IP" == ${EXPECTED_PREFIX}* ]]; then
            echo "‚ùå Error: IP '$IP' does not match VLAN $VLAN (expected ${EXPECTED_PREFIX}x)"
            exit 1
          fi
          echo "‚úì VLAN $VLAN and IP $IP are valid"

      - name: Terraform Init, Plan, and Apply
        if: ${{ inputs.skip_terraform != true }}
        run: |
          doppler run -- bash <<'EOF'
          set -e
          
          # Verify credentials are available
          if [ -z "$MINIO_ROOT_USER" ] || [ -z "$MINIO_ROOT_PASSWORD" ]; then
            echo "‚ùå ERROR: MINIO_ROOT_USER or MINIO_ROOT_PASSWORD not found in environment"
            exit 1
          fi
          
          cd terraform/
          
          # Set AWS credentials for Terraform S3 backend
          export AWS_ACCESS_KEY_ID="$MINIO_ROOT_USER"
          export AWS_SECRET_ACCESS_KEY="$MINIO_ROOT_PASSWORD"
          
          # Terraform Init
          terraform init \
            -backend-config="bucket=terraform-state" \
            -backend-config='endpoints={s3="http://192.168.20.10:9000"}' \
            -backend-config="access_key=$MINIO_ROOT_USER" \
            -backend-config="secret_key=$MINIO_ROOT_PASSWORD"
          
          # Select workspace
          terraform workspace select ${{ inputs.app_name }} || terraform workspace new ${{ inputs.app_name }}
          
          # Check if VM already exists (for retry scenarios)
          if terraform state list | grep -q "proxmox_vm_qemu.generic_vm"; then
            echo "‚ö†Ô∏è  VM already exists in Terraform state"
            echo "This is likely a retry after a previous failure"
            echo "Skipping Terraform apply, proceeding to Ansible configuration..."
            
            # Still need to ensure Tailscale key exists
            if ! terraform state list | grep -q "tailscale_tailnet_key.vm_auth_key"; then
              echo "Creating Tailscale auth key..."
              terraform apply -target=tailscale_tailnet_key.vm_auth_key -auto-approve \
                -var="app_name=${{ inputs.app_name }}" \
                -var="environment=${{ inputs.environment }}"
            fi
            
            # Skip to Ansible
            exit 0
          fi
          
          # Set Proxmox credentials
          export TF_VAR_proxmox_api_url="$PROXMOX_API_URL"
          export TF_VAR_proxmox_api_token_id="$PROXMOX_TOKEN_ID"
          export TF_VAR_proxmox_api_token_secret="$PROXMOX_TOKEN_SECRET"
          export TF_VAR_ssh_public_key="$ANS_SSH_PUBLIC_KEY"
          
          # Set Tailscale credentials for Terraform provider
          export TAILSCALE_OAUTH_CLIENT_ID="$TAILSCALE_OAUTH_CLIENT_ID"
          export TAILSCALE_OAUTH_CLIENT_SECRET="$TAILSCALE_OAUTH_CLIENT_SECRET"
          export TAILSCALE_TAILNET="${TAILSCALE_TAILNET:--}"
          
          # Create Tailscale cloud-init snippet
          TAILSCALE_AUTH_KEY=$(terraform output -raw tailscale_auth_key 2>/dev/null || echo "")
          if [ -z "$TAILSCALE_AUTH_KEY" ]; then
            # First run - need to create the key
            echo "Creating Tailscale auth key..."
          fi
          
          # Generate cloud-init snippet for Tailscale
          cat > /tmp/${{ inputs.app_name }}-tailscale.yml <<CLOUDINIT
          #cloud-config
          runcmd:
            - curl -fsSL https://tailscale.com/install.sh | sh
            - tailscale up --authkey=\${TAILSCALE_AUTH_KEY} --hostname=${{ inputs.app_name }} --accept-routes
            - timeout 30 sh -c 'until tailscale status --json | grep -q "\"Online\":true"; do sleep 2; done' || true
          CLOUDINIT
          
          # Upload to Proxmox snippets directory (requires SSH access or NFS mount)
          # For now, we'll note this in the output
          echo "üìù Cloud-init snippet created at /tmp/${{ inputs.app_name }}-tailscale.yml"
          echo "Note: Upload to Proxmox at /var/lib/vz/snippets/${{ inputs.app_name }}-tailscale.yml"
          
          # Terraform Plan
          terraform plan \
            -var="app_name=${{ inputs.app_name }}" \
            -var="environment=${{ inputs.environment }}" \
            -var="vlan_tag=${{ inputs.vlan_tag }}" \
            -var="vm_target_ip=${{ inputs.vm_target_ip }}" \
            -var="vm_cpu_cores=${{ inputs.cpu_cores }}" \
            -var="vm_ram_mb=${{ inputs.ram_mb }}" \
            -var="vm_disk_gb=${{ inputs.disk_gb }}" \
            -var="proxmox_target_node=${{ inputs.proxmox_node }}" \
            -var="proxmox_storage=${{ inputs.proxmox_storage }}" \
            -var="vm_template=${{ inputs.vm_template }}" \
            -var="resource_type=${{ inputs.resource_type }}" \
            -var="lxc_template=${{ inputs.lxc_template }}" \
            -var="lxc_nesting=${{ inputs.lxc_nesting }}" \
            -var="lxc_unprivileged=${{ inputs.lxc_unprivileged }}" \
            -var="enable_tailscale_terraform=false" \
            -out=tfplan
          
          # Terraform Apply
          if ! terraform apply tfplan; then
            echo "‚ùå Terraform apply failed"
            echo "If VM was partially created, you can:"
            echo "1. Re-run this workflow (it will detect existing VM and skip to Ansible)"
            echo "2. Or manually destroy: cd terraform && terraform destroy"
            exit 1
          fi
          
          # Get Tailscale auth key for Ansible
          export TAILSCALE_AUTH_KEY=$(terraform output -raw tailscale_auth_key 2>/dev/null || echo "")
          if [ -n "$TAILSCALE_AUTH_KEY" ]; then
            echo "TAILSCALE_AUTH_KEY=$TAILSCALE_AUTH_KEY" >> $GITHUB_ENV
          else
            echo "‚ö†Ô∏è  No Tailscale auth key found - Ansible will use fallback TS_AUTHKEY"
          fi
          EOF

      - name: Wait for VM to boot and SSH to be ready
        run: |
          echo "Waiting for ${{ inputs.resource_type }} at ${{ inputs.vm_target_ip }} to be reachable..."
          
          # Wait up to 3 minutes for the resource to respond to ping
          PING_SUCCESS=false
          MAX_ATTEMPTS=36
          # LXC containers boot much faster, reduce wait time
          if [ "${{ inputs.resource_type }}" = "lxc" ]; then
            MAX_ATTEMPTS=12  # 1 minute for LXC
          fi
          
          for i in $(seq 1 $MAX_ATTEMPTS); do
            if ping -c 1 -W 2 ${{ inputs.vm_target_ip }} &>/dev/null; then
              echo "‚úì ${{ inputs.resource_type }} is responding to ping (attempt $i)"
              PING_SUCCESS=true
              break
            fi
            echo "Attempt $i/$MAX_ATTEMPTS: ${{ inputs.resource_type }} not yet reachable, waiting 5s..."
            sleep 5
          done
          
          if [ "$PING_SUCCESS" = false ]; then
            echo "‚ùå ERROR: ${{ inputs.resource_type }} did not respond to ping"
            echo "Check that:"
            echo "  1. Resource was created successfully in Proxmox"
            if [ "${{ inputs.resource_type }}" = "vm" ]; then
              echo "  2. Cloud-init completed (check VM console)"
            fi
            echo "  3. Network configuration is correct (VLAN ${{ inputs.vlan_tag }})"
            echo "  4. IP ${{ inputs.vm_target_ip }} is not already in use"
            exit 1
          fi
          
          # Wait for SSH port to be open (up to 2 minutes)
          echo "Waiting for SSH port 22..."
          SSH_SUCCESS=false
          for i in {1..24}; do
            if nc -z -w 2 ${{ inputs.vm_target_ip }} 22 2>/dev/null; then
              echo "‚úì SSH port is open (attempt $i)"
              SSH_SUCCESS=true
              break
            fi
            echo "Attempt $i/24: SSH not ready, waiting 5s..."
            sleep 5
          done
          
          if [ "$SSH_SUCCESS" = false ]; then
            echo "‚ùå ERROR: SSH port did not open after 2 minutes"
            echo "Check that:"
            if [ "${{ inputs.resource_type }}" = "vm" ]; then
              echo "  1. Cloud-init completed successfully"
            fi
            echo "  2. SSH service is running"
            echo "  3. Firewall is not blocking port 22"
            exit 1
          fi
          
          # Additional wait for cloud-init to fully complete (VMs only)
          if [ "${{ inputs.resource_type }}" = "vm" ]; then
            echo "Waiting 15s for cloud-init to complete..."
            sleep 15
            echo "‚úì VM is ready for Ansible configuration"
          else
            echo "‚úì LXC container is ready for Ansible configuration"
          fi

      - name: Run Ansible with Doppler
        run: |
          doppler run -- bash <<'EOF'
          set -e
          
          # Setup SSH agent with key from Doppler
          eval $(ssh-agent -s)
          echo "$SSH_PRIVATE_KEY" | ssh-add -
          
          cd ansible/
          
          # Install requirements
          ansible-galaxy install -r requirements.yml
          
          # Clean up old SSH host keys to prevent conflicts
          echo "Cleaning up old SSH host keys for ${{ inputs.vm_target_ip }}..."
          ssh-keygen -R "${{ inputs.vm_target_ip }}" 2>/dev/null || true
          
          # Determine Ansible user based on resource type
          # VMs use cloud-init to create 'deploy' user
          # LXC containers only support SSH keys for root, so we run as root
          ANSIBLE_USER="deploy"
          if [ "${{ inputs.resource_type }}" = "lxc" ]; then
            echo "LXC detected - will run Ansible as root"
            ANSIBLE_USER="root"
          fi
          
          # Test connectivity
          echo "Testing Ansible connectivity to ${{ inputs.vm_target_ip }} as $ANSIBLE_USER..."
          if ! ansible all -i "${{ inputs.vm_target_ip }}," -m ping \
            --user $ANSIBLE_USER \
            --ssh-common-args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10'; then
            echo "‚ùå ERROR: Ansible cannot connect to ${{ inputs.resource_type }}"
            exit 1
          fi
          echo "‚úì Ansible connectivity confirmed"
          
          # Parse Octopus roles from comma-separated string to JSON array
          OCTOPUS_ROLES_JSON="[]"
          if [ -n "${{ inputs.octopus_roles }}" ]; then
            # Simple bash parsing - no jq needed
            IFS=',' read -ra ROLES <<< "${{ inputs.octopus_roles }}"
            OCTOPUS_ROLES_JSON="["
            for i in "${!ROLES[@]}"; do
              ROLE=$(echo "${ROLES[$i]}" | xargs)  # trim whitespace
              if [ -n "$ROLE" ]; then
                [ "$i" -gt 0 ] && OCTOPUS_ROLES_JSON+=","
                OCTOPUS_ROLES_JSON+="\"$ROLE\""
              fi
            done
            OCTOPUS_ROLES_JSON+="]"
          fi
          
          # Run playbook with all integrations (Tailscale, LDAP, Octopus, Grafana)
          # Note: LXC runs as root, VMs run as deploy user (created by cloud-init)
          echo "Running Ansible playbook as $ANSIBLE_USER..."
          ansible-playbook -i "${{ inputs.vm_target_ip }}," site.yml \
            --user $ANSIBLE_USER \
            --ssh-common-args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' \
            --extra-vars "target_hostname=${{ inputs.app_name }}" \
            --extra-vars "vlan_tag=${{ inputs.vlan_tag }}" \
            --extra-vars "app_role_name=${{ inputs.app_name }}" \
            --extra-vars "octopus_environment=${{ inputs.octopus_environment }}" \
            --extra-vars "octopus_roles=$OCTOPUS_ROLES_JSON" \
            --extra-vars "freeipa_server_ip=${FREEIPA_SERVER_IP:-}" \
            --extra-vars "freeipa_admin_password=${FREEIPA_ADMIN_PASSWORD:-}"
          
          echo "‚úì VM provisioned and configured"
          echo "‚úì Octopus Tentacle installed and registered"
          echo ""
          echo "üìã Next steps:"
          echo "  1. Check Tailscale admin console - device should appear within 1-2 minutes"
          echo "  2. Check Octopus Deploy - target should be healthy"
          echo "  3. SSH to VM: ssh deploy@${{ inputs.vm_target_ip }}"
          EOF