# ‚ö†Ô∏è SECURITY NOTICE:
# This workflow defaults to GitHub-hosted runners for safety in public repositories.
# Only use runner_type: "self-hosted" in PRIVATE repositories with trusted contributors.
# Fork PRs will require manual approval before running infrastructure operations.

name: "Reusable VM Destruction"

permissions:
  contents: read

on:
  workflow_call:
    inputs:
      app_name:
        type: string
        description: 'Application Name (e.g. nexus, k3s, octopus)'
        required: true
      confirm_destroy:
        type: boolean
        description: 'Confirm destruction (set to true to actually destroy)'
        required: true
        default: false
      vlan_tag:
        type: string
        description: 'Network Zone'
        default: '20'
      cpu_cores:
        type: string
        description: 'CPU Cores'
        default: '2'
      ram_mb:
        type: string
        description: 'RAM in MB'
        default: '4096'
      disk_gb:
        type: string
        description: 'Disk Size (e.g. 20G)'
        default: '20G'
      vm_target_ip:
        type: string
        description: 'IP of the VM to destroy (must match VLAN subnet) - for single instance mode'
        required: false
        default: ''
      instances:
        type: string
        description: 'JSON map of instances for multi-instance deployments (e.g., {"node1":{"ip_address":"192.168.10.51"}}). Leave empty for single instance mode.'
        required: false
        default: ''
      environment:
        type: string
        description: 'Environment name (dev, staging, prod)'
        required: false
        default: 'dev'
      resource_type:
        type: string
        description: 'Resource type: vm or lxc'
        required: false
        default: 'vm'
      # Runner configuration
      runner_type:
        type: string
        description: 'Runner type: "github-hosted" (uses ubuntu-latest with Tailscale) or "self-hosted" (uses self-hosted runner with direct LAN access). ‚ö†Ô∏è Use self-hosted ONLY in private repositories.'
        required: false
        default: 'github-hosted'
      tailscale_tags:
        type: string
        description: 'Tailscale ACL tags for GitHub-hosted runner (comma-separated, e.g., "tag:ci,tag:provision")'
        required: false
        default: 'tag:ci'
    secrets:
      DOPPLER_TOKEN:
        required: true
      DOPPLER_TARGET_PROJECT:
        required: true
      DOPPLER_TARGET_CONFIG:
        required: true
      GH_PAT:
        required: true
      # Optional: Required for GitHub-hosted runners
      TS_OAUTH_CLIENT_ID:
        required: false
      TS_OAUTH_CLIENT_SECRET:
        required: false

jobs:
  check-runner-access:
    runs-on: ubuntu-latest
    outputs:
      runner: ${{ steps.select-runner.outputs.runner }}
    steps:
      - name: Select runner based on org membership
        id: select-runner
        uses: actions/github-script@v7
        with:
          script: |
            const requestedRunner = '${{ inputs.runner_type }}';
            
            // Default to github-hosted
            let runner = 'ubuntu-latest';
            
            // Block fork PRs from self-hosted
            if (context.eventName === 'pull_request' || context.eventName === 'pull_request_target') {
              const prRepo = context.payload.pull_request?.head?.repo?.full_name;
              if (prRepo !== `${context.repo.owner}/${context.repo.repo}`) {
                core.warning('Fork PRs use GitHub-hosted runners only');
                core.setOutput('runner', 'ubuntu-latest');
                return;
              }
            }
            
            // If self-hosted requested, verify org membership
            if (requestedRunner === 'self-hosted') {
              if (context.repo.owner !== 'KoraMaple') {
                core.warning('Self-hosted only available for KoraMaple org repos');
                core.setOutput('runner', 'ubuntu-latest');
                return;
              }
              
              try {
                const { data } = await github.rest.orgs.getMembershipForUser({
                  org: 'KoraMaple',
                  username: context.actor
                });
                if (data.state === 'active') {
                  core.info(`‚úì ${context.actor} is KoraMaple org member - self-hosted allowed`);
                  runner = 'self-hosted';
                }
              } catch (e) {
                core.warning(`${context.actor} is not a KoraMaple org member - using GitHub-hosted`);
              }
            }
            
            core.setOutput('runner', runner);

  destroy:
    needs: check-runner-access
    runs-on: ${{ needs.check-runner-access.outputs.runner }}
    # Prevent fork PRs from running infrastructure operations - maintainers must explicitly trigger
    if: |
      github.event_name != 'pull_request' ||
      github.event.pull_request.head.repo.full_name == github.repository
    
    steps:
      # Connect to Tailscale for GitHub-hosted runners
      - name: Connect to Tailscale
        if: needs.check-runner-access.outputs.runner == 'ubuntu-latest'
        uses: tailscale/github-action@v4
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_CLIENT_SECRET }}
          tags: ${{ inputs.tailscale_tags }}

      - name: Checkout Reusable Workflow Repo
        uses: actions/checkout@v4
        with:
          repository: KoraMaple/nante-reusable-workflow
          ref: develop
          token: ${{ secrets.GH_PAT }}

      - name: Install Doppler CLI
        uses: dopplerhq/cli-action@v3
      
      - name: Configure Doppler
        run: |
          doppler setup --project "${{ secrets.DOPPLER_TARGET_PROJECT }}" --config "${{ secrets.DOPPLER_TARGET_CONFIG }}" --no-interactive
        env:
          DOPPLER_TOKEN: ${{ secrets.DOPPLER_TOKEN }}

      - name: Confirm Destruction
        run: |
          if [ "${{ inputs.confirm_destroy }}" != "true" ]; then
            echo "‚ùå Destruction aborted. Set 'confirm_destroy' to true to proceed."
            exit 1
          fi
          echo "‚úì Destruction confirmed for app: ${{ inputs.app_name }}"

      - name: Terraform Destroy with Doppler
        run: |
          doppler run -- bash <<'EOF'
          set -e
          
          # Mask all sensitive values to prevent log exposure
          [ -n "$MINIO_ROOT_PASSWORD" ] && echo "::add-mask::$MINIO_ROOT_PASSWORD"
          [ -n "$PROXMOX_TOKEN_SECRET" ] && echo "::add-mask::$PROXMOX_TOKEN_SECRET"
          [ -n "$TAILSCALE_OAUTH_CLIENT_SECRET" ] && echo "::add-mask::$TAILSCALE_OAUTH_CLIENT_SECRET"
          [ -n "$SSH_PRIVATE_KEY" ] && echo "::add-mask::$SSH_PRIVATE_KEY"
          [ -n "$ANS_SSH_PUBLIC_KEY" ] && echo "::add-mask::$ANS_SSH_PUBLIC_KEY"
          [ -n "$OCTOPUS_API_KEY" ] && echo "::add-mask::$OCTOPUS_API_KEY"
          [ -n "$TS_AUTHKEY" ] && echo "::add-mask::$TS_AUTHKEY"
          [ -n "$TAILSCALE_AUTH_KEY" ] && echo "::add-mask::$TAILSCALE_AUTH_KEY"
          
          cd terraform/
          
          # Set AWS credentials for Terraform S3 backend
          export AWS_ACCESS_KEY_ID="$MINIO_ROOT_USER"
          export AWS_SECRET_ACCESS_KEY="$MINIO_ROOT_PASSWORD"
          
          # Set Proxmox credentials
          export TF_VAR_proxmox_api_url="$PROXMOX_API_URL"
          export TF_VAR_proxmox_api_token_id="$PROXMOX_TOKEN_ID"
          export TF_VAR_proxmox_api_token_secret="$PROXMOX_TOKEN_SECRET"
          export TF_VAR_ssh_public_key="$ANS_SSH_PUBLIC_KEY"
          
          # Set Tailscale credentials for Terraform provider
          export TAILSCALE_OAUTH_CLIENT_ID="$TAILSCALE_OAUTH_CLIENT_ID"
          export TAILSCALE_OAUTH_CLIENT_SECRET="$TAILSCALE_OAUTH_CLIENT_SECRET"
          export TAILSCALE_TAILNET="${TAILSCALE_TAILNET:--}"
          
          # Use MinIO endpoint from Doppler (required secret)
          if [ -z "$MINIO_ENDPOINT" ]; then
            echo "‚ùå ERROR: MINIO_ENDPOINT not found in Doppler. Please configure this secret."
            exit 1
          fi
          
          # Terraform Init
          terraform init \
            -backend-config="bucket=terraform-state" \
            -backend-config="endpoints={s3=\"${MINIO_ENDPOINT}\"}" \
            -backend-config="access_key=$MINIO_ROOT_USER" \
            -backend-config="secret_key=$MINIO_ROOT_PASSWORD"
          
          # Select workspace
          terraform workspace select ${{ inputs.app_name }}
          
          # List available workspaces for debugging
          echo "Available workspaces:"
          terraform workspace list
          
          # Check if current workspace matches app_name
          CURRENT_WS=$(terraform workspace show)
          if [ "$CURRENT_WS" != "${{ inputs.app_name }}" ]; then
            echo "‚ùå Workspace '${{ inputs.app_name }}' does not exist."
            echo "   This means no infrastructure was provisioned with this app name."
            exit 1
          fi
          
          # Set Proxmox credentials
          export TF_VAR_proxmox_api_url="$PROXMOX_API_URL"
          export TF_VAR_proxmox_api_token_id="$PROXMOX_TOKEN_ID"
          echo "Destroying infrastructure for app: ${{ inputs.app_name }}"
          echo "Environment: ${{ inputs.environment }}"
          echo "VLAN: ${{ inputs.vlan_tag }}"
          
          # Build destroy arguments based on deployment mode
          DESTROY_ARGS=(
            -auto-approve
            -var="app_name=${{ inputs.app_name }}"
            -var="environment=${{ inputs.environment }}"
            -var="vlan_tag=${{ inputs.vlan_tag }}"
            -var="vm_cpu_cores=${{ inputs.cpu_cores }}"
            -var="vm_ram_mb=${{ inputs.ram_mb }}"
            -var="vm_disk_gb=${{ inputs.disk_gb }}"
            -var="resource_type=${{ inputs.resource_type }}"
            -var="enable_tailscale_terraform=false"
          )
          
          # Add instances or vm_target_ip based on mode
          if [ -n "${{ inputs.instances }}" ]; then
            echo "Multi-instance mode"
            echo '${{ inputs.instances }}' > /tmp/instances.json
            DESTROY_ARGS+=(-var="instances=$(cat /tmp/instances.json)")
            echo "Instances to destroy:"
            cat /tmp/instances.json | jq -r 'to_entries[] | "  \(.key): \(.value.ip_address)"'
          else
            echo "Single-instance mode"
            echo "Target IP: ${{ inputs.vm_target_ip }}"
            DESTROY_ARGS+=(-var="vm_target_ip=${{ inputs.vm_target_ip }}")
          fi
          echo ""
          
          # Terraform Destroy
          terraform destroy "${DESTROY_ARGS[@]}"
          echo "‚úì Infrastructure destroyed successfully"
          if [ -n "${{ inputs.instances }}" ]; then
            echo "‚úì All instances removed from Proxmox"
          else
            echo "‚úì VM/LXC removed from Proxmox"
          fi
          echo "‚úì Tailscale devices cleaned up (if Terraform-managed)"
          echo "‚úì Terraform state cleaned up"
          
          # Note about manual cleanup
          echo ""
          echo "üìã Manual cleanup (if needed):"
          echo "  1. Remove Octopus Deploy targets manually if they were registered"
          echo "  2. Check Tailscale admin console for any orphaned devices"
          echo "  3. Verify Proxmox has no leftover resources"
          EOF
